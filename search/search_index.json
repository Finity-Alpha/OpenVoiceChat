{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"Adding_models/","title":"Adding custom models","text":"<p>One of the goals of open voice chat is for it to be very easy to add new models to the system. The code is meant to be process level not object level. We have made it so that you can add your own models to the system with very little effort, so when the models  improve so does the overall system.</p>"},{"location":"Adding_models/#stt","title":"STT","text":"<p>To add a new STT model you need to create a new class that inherits from the <code>BaseEar</code> class. This class should implement the <code>transcribe</code> method that takes in an audio np array and returns a string.</p> <p>Here is how the huggingface model is implemented:</p> <pre><code>class Ear_hf(BaseEar):\n    def __init__(self, model_id='openai/whisper-base.en', device='cpu',\n                 silence_seconds=2, generate_kwargs=None):\n        super().__init__(silence_seconds, stream=False)\n        self.pipe = pipeline('automatic-speech-recognition', model=model_id, device=device)\n        self.device = device\n        self.generate_kwargs = generate_kwargs\n\n    @torch.no_grad()\n    def transcribe(self, audio):\n        transcription = self.pipe(audio, generate_kwargs=self.generate_kwargs)\n        return transcription['text'].strip()\n</code></pre>"},{"location":"Adding_models/#tts","title":"TTS","text":"<p>To add a new TTS model you need to create a new class that inherits from the <code>BaseMouth</code> class. This class should implement the <code>run_tts</code> method that takes in a string and returns an audio np array.</p> <p>Here is how the piper-tts model is implemented: <pre><code>class Mouth_piper(BaseMouth):\n    def __init__(self, device='cpu', model_path='models/en_US-ryan-high.onnx',\n                 config_path='models/en_en_US_ryan_high_en_US-ryan-high.onnx.json'):\n        self.model = piper.PiperVoice.load(model_path=model_path,\n                                           config_path=config_path,\n                                           use_cuda=True if device == 'cuda' else False)\n        super().__init__(sample_rate=self.model.config.sample_rate)\n\n    def run_tts(self, text):\n        audio = b''\n        for i in self.model.synthesize_stream_raw(text):\n            audio += i\n        return np.frombuffer(audio, dtype=np.int16)\n</code></pre></p>"},{"location":"Adding_models/#llm","title":"LLM","text":"<p>To add a new LLM model you need to create a new class that inherits from the <code>BaseChatbot</code> class. This class should implement the <code>run</code> method that takes in the user message and yields the assistant's response, and the <code>post_process</code> method that takes in the response of the assistant does any post-processing and returns it.</p> <p>Here is how the gpt model is implemented: <pre><code>class Chatbot_gpt(BaseChatbot):\n    def __init__(self, sys_prompt='', Model='gpt-3.5-turbo'):\n        load_dotenv()\n        OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n        self.MODEL = Model\n        self.client = OpenAI(api_key=OPENAI_API_KEY)\n        self.messages = []\n        self.messages.append({\"role\": \"system\", \"content\": sys_prompt})\n\n    def run(self, input_text):\n        self.messages.append({\"role\": \"user\", \"content\": input_text})\n        stream = self.client.chat.completions.create(\n            model=self.MODEL,\n            messages=self.messages,\n            stream=True,\n        )\n        for chunk in stream:\n            if chunk.choices[0].delta.content is not None:\n                yield chunk.choices[0].delta.content\n\n    def post_process(self, response):\n        self.messages.append({\"role\": \"assistant\", \"content\": response})\n        return response\n</code></pre></p>"},{"location":"Advanced_Usage/","title":"Advanced Usage","text":"<p>Tips and tricks for getting the most out of OpenVoiceChat.</p>"},{"location":"Advanced_Usage/#key-words","title":"[Key words]","text":"<p>Text surrounded in square brackets is not spoken by the tts model. This form of \"tokens\" can be used for stopping criteria, special commands, or other purposes.</p>"},{"location":"Advanced_Usage/#stopping-criteria","title":"Stopping criteria","text":"<p>The <code>run_chat</code>(inside utils) function takes in a stopping criteria function that is called at the end of each turn. This function should return a boolean value indicating whether the chat should continue. By default, the stopping criteria function is <code>lambda x: False</code>, which does not stop the chat. You can pass in your own stopping criteria function to <code>run_chat</code> to customize when the chat should stop. For example <code>lambda x: \"[END]\" in x</code> will stop the chat when the model outputs the string \"[END]\".</p>"},{"location":"Advanced_Usage/#integration-with-other-audio-streams","title":"Integration with other audio streams","text":"<p>The <code>player</code> and <code>listener</code> classes are used to connect the audio pipeline with external audio streams. See the twilio or websocket player and listener for an example of how to integrate with external audio streams.</p>"},{"location":"base_classes/","title":"Base Classes","text":"<p>The base classes are responsible for running the core functionality such as streaming, multiprocessing and other parallelization.</p>"},{"location":"base_classes/#stt.base.BaseEar","title":"stt.base.BaseEar","text":"<pre><code>BaseEar(silence_seconds=2, not_interrupt_words=None, listener=None, stream=False, listen_interruptions=True, logger=None)\n</code></pre> <p>Initializes the BaseEar class.</p> <p>Parameters:</p> Name Type Description Default <code>silence_seconds</code> <code>float, optional</code> <p>Number of seconds of silence to detect. Defaults to 2.</p> <code>2</code> <code>not_interrupt_words</code> <code>list, optional</code> <p>List of words that should not be considered as interruptions.</p> <code>None</code> <code>listener</code> <code>object, optional</code> <p>Listener object to receive the audio from. Defaults to None.</p> <code>None</code> <code>stream</code> <code>bool, optional</code> <p>Flag indicating whether to stream the audio or process it as a whole. Defaults to False.</p> <code>False</code> <code>listen_interruptions</code> <code>bool, optional</code> <p>Flag indicating whether to listen for interruptions. Defaults to True.</p> <code>True</code> <p>Methods:</p> Name Description <code>transcribe</code> <p>Given an audio input, return the transcription</p> <code>transcribe_stream</code> <p>:param audio_queue: Queue containing audio chunks from pyaudio stream</p> <code>listen</code> <p>records audio using record_user and returns its transcription</p> <code>interrupt_listen</code> <p>Records audio with interruption. Transcribes audio if</p> <p>Attributes:</p> Name Type Description <code>silence_seconds</code> <code>not_interrupt_words</code> <code>vad</code> <code>listener</code> <code>stream</code> <code>listen_interruptions</code> <code>logger</code> Source code in <code>openvoicechat/stt/base.py</code> <pre><code>def __init__(\n    self,\n    silence_seconds=2,\n    not_interrupt_words=None,\n    listener=None,\n    stream=False,\n    listen_interruptions=True,\n    logger=None,\n):\n    \"\"\"\n    Initializes the BaseEar class.\n\n    :param silence_seconds: Number of seconds of silence to detect. Defaults to 2.\n    :type silence_seconds: float, optional\n    :param not_interrupt_words: List of words that should not be considered as interruptions.\n    :type not_interrupt_words: list, optional\n    :param listener: Listener object to receive the audio from. Defaults to None.\n    :type listener: object, optional\n    :param stream: Flag indicating whether to stream the audio or process it as a whole. Defaults to False.\n    :type stream: bool, optional\n    :param listen_interruptions: Flag indicating whether to listen for interruptions. Defaults to True.\n    :type listen_interruptions: bool, optional\n    \"\"\"\n\n    if not_interrupt_words is None:\n        not_interrupt_words = [\n            \"you\",\n            \"yes\",\n            \"yeah\",\n            \"hmm\",\n        ]  # you because whisper says \"you\" in silence\n    self.silence_seconds = silence_seconds\n    self.not_interrupt_words = not_interrupt_words\n    self.vad = VoiceActivityDetection()\n    self.listener = listener\n    self.stream = stream\n    self.listen_interruptions = listen_interruptions\n    self.logger = logger\n</code></pre>"},{"location":"base_classes/#stt.base.BaseEar.silence_seconds","title":"silence_seconds  <code>instance-attribute</code>","text":"<pre><code>silence_seconds = silence_seconds\n</code></pre>"},{"location":"base_classes/#stt.base.BaseEar.not_interrupt_words","title":"not_interrupt_words  <code>instance-attribute</code>","text":"<pre><code>not_interrupt_words = not_interrupt_words\n</code></pre>"},{"location":"base_classes/#stt.base.BaseEar.vad","title":"vad  <code>instance-attribute</code>","text":"<pre><code>vad = VoiceActivityDetection()\n</code></pre>"},{"location":"base_classes/#stt.base.BaseEar.listener","title":"listener  <code>instance-attribute</code>","text":"<pre><code>listener = listener\n</code></pre>"},{"location":"base_classes/#stt.base.BaseEar.stream","title":"stream  <code>instance-attribute</code>","text":"<pre><code>stream = stream\n</code></pre>"},{"location":"base_classes/#stt.base.BaseEar.listen_interruptions","title":"listen_interruptions  <code>instance-attribute</code>","text":"<pre><code>listen_interruptions = listen_interruptions\n</code></pre>"},{"location":"base_classes/#stt.base.BaseEar.logger","title":"logger  <code>instance-attribute</code>","text":"<pre><code>logger = logger\n</code></pre>"},{"location":"base_classes/#stt.base.BaseEar.transcribe","title":"transcribe","text":"<pre><code>transcribe(input_audio: np.ndarray) -&gt; str\n</code></pre> <p>Given an audio input, return the transcription</p> <p>Parameters:</p> Name Type Description Default <code>input_audio</code> <code>ndarray</code> required <p>Returns:</p> Type Description <code>str</code> <p>transcription</p> Source code in <code>openvoicechat/stt/base.py</code> <pre><code>def transcribe(self, input_audio: np.ndarray) -&gt; str:\n    \"\"\"\n    Given an audio input, return the transcription\n    :param input_audio:\n    :return: transcription\n    \"\"\"\n    raise NotImplementedError(\"This method should be implemented by the subclass\")\n</code></pre>"},{"location":"base_classes/#stt.base.BaseEar.transcribe_stream","title":"transcribe_stream","text":"<pre><code>transcribe_stream(audio_queue: Queue, transcription_queue: Queue)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>audio_queue</code> <code>Queue</code> <p>Queue containing audio chunks from pyaudio stream</p> required <code>transcription_queue</code> <code>Queue</code> <p>Queue to put transcriptions</p> required Source code in <code>openvoicechat/stt/base.py</code> <pre><code>def transcribe_stream(self, audio_queue: Queue, transcription_queue: Queue):\n    \"\"\"\n    :param audio_queue: Queue containing audio chunks from pyaudio stream\n    :param transcription_queue: Queue to put transcriptions\n    \"\"\"\n    raise NotImplementedError(\"This method should be implemented by the subclass\")\n</code></pre>"},{"location":"base_classes/#stt.base.BaseEar._sim_transcribe_stream","title":"_sim_transcribe_stream","text":"<pre><code>_sim_transcribe_stream(input_audio: np.ndarray) -&gt; str\n</code></pre> <p>Simulates the transcribe stream using a single audio input</p> <p>Parameters:</p> Name Type Description Default <code>input_audio</code> <code>ndarray</code> <p>fp32 numpy array of the audio</p> required <p>Returns:</p> Type Description <code>str</code> <p>transcription</p> Source code in <code>openvoicechat/stt/base.py</code> <pre><code>def _sim_transcribe_stream(self, input_audio: np.ndarray) -&gt; str:\n    \"\"\"\n    Simulates the transcribe stream using a single audio input\n    :param input_audio: fp32 numpy array of the audio\n    :return: transcription\n    \"\"\"\n    audio_queue = Queue()\n    transcription_queue = Queue()\n\n    input_buffer = (input_audio * (1 &lt;&lt; 15)).astype(np.int16).tobytes()\n    audio_queue.put(input_buffer)\n    audio_queue.put(None)\n    transcription_thread = Thread(\n        target=self.transcribe_stream, args=(audio_queue, transcription_queue)\n    )\n    transcription_thread.start()\n    transcription_thread.join()\n    text = \"\"\n    while True:\n        _ = transcription_queue.get()\n        if _ is None:\n            break\n        text += _ + \" \"\n    return text\n</code></pre>"},{"location":"base_classes/#stt.base.BaseEar._log_event","title":"_log_event","text":"<pre><code>_log_event(event: str, details: str, further: str = '')\n</code></pre> Source code in <code>openvoicechat/stt/base.py</code> <pre><code>def _log_event(self, event: str, details: str, further: str = \"\"):\n    if self.logger:\n        self.logger.info(\n            event, extra={\"details\": details, \"further\": f'\"{further}\"'}\n        )\n</code></pre>"},{"location":"base_classes/#stt.base.BaseEar._listen","title":"_listen","text":"<pre><code>_listen() -&gt; str\n</code></pre> <p>records audio using record_user and returns its transcription</p> <p>Returns:</p> Type Description <code>str</code> <p>transcription</p> Source code in <code>openvoicechat/stt/base.py</code> <pre><code>def _listen(self) -&gt; str:\n    \"\"\"\n    records audio using record_user and returns its transcription\n    :return: transcription\n    \"\"\"\n    import pysbd\n\n    seg = pysbd.Segmenter(language=\"en\", clean=False)\n\n    sentence_finished = False\n    first = True\n    audio = np.zeros(0, dtype=np.float32)\n    n = 2  # number of times to see if the sentence ends\n    while not sentence_finished and n &gt; 0:\n\n        new_audio = record_user(\n            self.silence_seconds,\n            self.vad,\n            self.listener,\n            started=not first,\n            logger=self.logger,\n        )\n\n        audio = np.concatenate((audio, new_audio), 0)\n\n        self._log_event(\"transcribing\", \"STT\")\n        text = self.transcribe(audio)\n        self._log_event(\"transcribed\", \"STT\", text)\n\n        self._log_event(\"segmenting\", \"STT\", text)\n        first = False\n        if len(seg.segment(text + \" .\")) &gt; 1:\n            sentence_finished = True\n            self._log_event(\"sentence boundary detected\", \"STT\", text)\n        else:\n            n -= 1\n            self._log_event(\n                \"no sentence boundary detected\",\n                \"STT\",\n                text + \". tries left: \" + str(n),\n            )\n    return text\n</code></pre>"},{"location":"base_classes/#stt.base.BaseEar._listen_stream","title":"_listen_stream","text":"<pre><code>_listen_stream() -&gt; str\n</code></pre> <p>records audio using record_user and returns its transcription</p> <p>Returns:</p> Type Description <code>str</code> <p>transcription</p> Source code in <code>openvoicechat/stt/base.py</code> <pre><code>def _listen_stream(self) -&gt; str:\n    \"\"\"\n    records audio using record_user and returns its transcription\n    :return: transcription\n    \"\"\"\n\n    audio_queue = Queue()\n    transcription_queue = Queue()\n\n    audio_thread = Thread(\n        target=record_user_stream,\n        args=(self.silence_seconds, self.vad, audio_queue, self.listener),\n    )\n    transcription_thread = Thread(\n        target=self.transcribe_stream, args=(audio_queue, transcription_queue)\n    )\n\n    audio_thread.start()\n    transcription_thread.start()\n\n    text = \"\"\n    while True:\n        _ = transcription_queue.get()\n        if _ is None:\n            break\n        text += _ + \" \"\n    audio_thread.join()\n    transcription_thread.join()\n    return text\n</code></pre>"},{"location":"base_classes/#stt.base.BaseEar.listen","title":"listen","text":"<pre><code>listen() -&gt; str\n</code></pre> <p>records audio using record_user and returns its transcription</p> <p>Returns:</p> Type Description <code>str</code> <p>transcription</p> Source code in <code>openvoicechat/stt/base.py</code> <pre><code>def listen(self) -&gt; str:\n    \"\"\"\n    records audio using record_user and returns its transcription\n    :return: transcription\n    \"\"\"\n    if self.stream:\n        return self._listen_stream()\n    else:\n        return self._listen()\n</code></pre>"},{"location":"base_classes/#stt.base.BaseEar.interrupt_listen","title":"interrupt_listen","text":"<pre><code>interrupt_listen(record_seconds=100) -&gt; str\n</code></pre> <p>Records audio with interruption. Transcribes audio if voice activity detected and returns True if transcription indicates interruption.</p> <p>Parameters:</p> Name Type Description Default <code>record_seconds</code> <p>Max seconds to record for</p> <code>100</code> <p>Returns:</p> Type Description <code>str</code> <p>boolean indicating the if an interruption occured</p> Source code in <code>openvoicechat/stt/base.py</code> <pre><code>def interrupt_listen(self, record_seconds=100) -&gt; str:\n    \"\"\"\n    Records audio with interruption. Transcribes audio if\n    voice activity detected and returns True if transcription indicates\n    interruption.\n\n    :param record_seconds: Max seconds to record for\n    :return: boolean indicating the if an interruption occured\n    \"\"\"\n    if not self.listen_interruptions:\n        return False\n    while record_seconds &gt; 0:\n        interruption_audio = record_interruption(\n            self.vad, record_seconds, streamer=self.listener, logger=self.logger\n        )\n        # duration of interruption audio\n        if interruption_audio is None:\n            return \"\"\n        else:\n            duration = len(interruption_audio) / 16_000\n            self._log_event(\n                \"transcribing interruption\", \"STT\", f\"{duration} seconds\"\n            )\n            if self.stream:\n                text = self._sim_transcribe_stream(interruption_audio)\n            else:\n                text = self.transcribe(interruption_audio)\n            self._log_event(\"interruption transcribed\", \"STT\", text)\n            # remove any punctuation using re\n            text = re.sub(r\"[^\\w\\s]\", \"\", text)\n            text = text.lower()\n            text = text.strip()\n            if text in self.not_interrupt_words:\n                self._log_event(\"not interruption\", \"STT\", text)\n                record_seconds -= duration\n            else:\n                return text\n</code></pre>"},{"location":"base_classes/#llm.base.BaseChatbot","title":"llm.base.BaseChatbot","text":"<pre><code>BaseChatbot(logger=None)\n</code></pre> <p>Initialize the model and other things here</p> <p>Methods:</p> Name Description <code>run</code> <p>Yields the response to the input text</p> <code>post_process</code> <p>Post process the response before returning</p> <code>generate_response</code> <p>:param input_text: The user input</p> <code>generate_response_stream</code> <p>:param input_text: The user input</p> <p>Attributes:</p> Name Type Description <code>logger</code> Source code in <code>openvoicechat/llm/base.py</code> <pre><code>def __init__(self, logger=None):\n    \"\"\"\n    Initialize the model and other things here\n    \"\"\"\n    self.logger = logger\n</code></pre>"},{"location":"base_classes/#llm.base.BaseChatbot.logger","title":"logger  <code>instance-attribute</code>","text":"<pre><code>logger = logger\n</code></pre>"},{"location":"base_classes/#llm.base.BaseChatbot.run","title":"run","text":"<pre><code>run(input_text: str)\n</code></pre> <p>Yields the response to the input text</p> Source code in <code>openvoicechat/llm/base.py</code> <pre><code>def run(self, input_text: str):\n    \"\"\"\n    Yields the response to the input text\n    \"\"\"\n    raise NotImplementedError(\"This method should be implemented by the subclass\")\n</code></pre>"},{"location":"base_classes/#llm.base.BaseChatbot.post_process","title":"post_process","text":"<pre><code>post_process(response: str) -&gt; str\n</code></pre> <p>Post process the response before returning</p> Source code in <code>openvoicechat/llm/base.py</code> <pre><code>def post_process(self, response: str) -&gt; str:\n    \"\"\"\n    Post process the response before returning\n    \"\"\"\n    return response\n</code></pre>"},{"location":"base_classes/#llm.base.BaseChatbot.generate_response","title":"generate_response","text":"<pre><code>generate_response(input_text: str) -&gt; str\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>input_text</code> <code>str</code> <p>The user input</p> required <p>Returns:</p> Type Description <code>str</code> <p>The chatbot's response.</p> Source code in <code>openvoicechat/llm/base.py</code> <pre><code>def generate_response(self, input_text: str) -&gt; str:\n    \"\"\"\n    :param input_text: The user input\n    :return: The chatbot's response.\n    \"\"\"\n    out = self.run(input_text)\n    response_text = \"\"\n    for o in out:\n        text = o\n        response_text += text\n    response = self.post_process(response_text)\n    return response\n</code></pre>"},{"location":"base_classes/#llm.base.BaseChatbot._log_event","title":"_log_event","text":"<pre><code>_log_event(event: str, details: str, further: str)\n</code></pre> Source code in <code>openvoicechat/llm/base.py</code> <pre><code>def _log_event(self, event: str, details: str, further: str):\n    if self.logger:\n        self.logger.info(event, extra={\"details\": details, \"further\": further})\n</code></pre>"},{"location":"base_classes/#llm.base.BaseChatbot.generate_response_stream","title":"generate_response_stream","text":"<pre><code>generate_response_stream(input_text: str, output_queue: queue.Queue, interrupt_queue: queue.Queue) -&gt; str\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>input_text</code> <code>str</code> <p>The user input</p> required <code>output_queue</code> <code>Queue</code> <p>The text output queue where the result is accumulated.</p> required <code>interrupt_queue</code> <code>Queue</code> <p>The interrupt queue which stores the transcription if interruption occurred. Used to stop generating.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The chatbot's response after running self.post_process</p> Source code in <code>openvoicechat/llm/base.py</code> <pre><code>def generate_response_stream(\n    self, input_text: str, output_queue: queue.Queue, interrupt_queue: queue.Queue\n) -&gt; str:\n    \"\"\"\n    :param input_text: The user input\n    :param output_queue: The text output queue where the result is accumulated.\n    :param interrupt_queue: The interrupt queue which stores the transcription if interruption occurred. Used to stop generating.\n    :return: The chatbot's response after running self.post_process\n    \"\"\"\n    self._log_event(\"llm request sent\", \"LLM\", \"\")\n    out = self.run(input_text)\n    response_text = \"\"\n    for text in out:\n        if not interrupt_queue.empty():\n            self._log_event(\"interruption detected\", \"LLM\", \"\")\n            break\n        self._log_event(\"llm token received\", \"LLM\", f'\"{text}\"')\n        output_queue.put(text)\n        response_text += text\n    output_queue.put(None)\n    self._log_event(\"llm post processing\", \"LLM\", \"\")\n    response = self.post_process(response_text)\n    return response\n</code></pre>"},{"location":"base_classes/#tts.base.BaseMouth","title":"tts.base.BaseMouth","text":"<pre><code>BaseMouth(sample_rate: int, player=sd, wait=True, logger=None)\n</code></pre> <p>Initializes the BaseMouth class.</p> <p>Parameters:</p> Name Type Description Default <code>sample_rate</code> <code>int</code> <p>The sample rate of the audio.</p> required <code>player</code> <p>The audio player object. Defaults to sounddevice.</p> <code>sounddevice</code> <code>wait</code> <p>Whether to wait for the audio to finish playing. Defaults to True.</p> <code>True</code> <p>Methods:</p> Name Description <code>run_tts</code> <p>:param text: The text to synthesize speech for</p> <code>say_text</code> <p>calls run_tts and plays the audio using the player.</p> <code>say</code> <p>Plays the audios in the queue using the player. Stops if interruption occurred.</p> <code>say_multiple</code> <p>Splits the text into sentences. Then plays the sentences one by one</p> <code>say_multiple_stream</code> <p>Receives text from the text_queue. As soon as a sentence is made run_tts is called to</p> <p>Attributes:</p> Name Type Description <code>sample_rate</code> <code>interrupted</code> <code>player</code> <code>seg</code> <code>wait</code> <code>logger</code> Source code in <code>openvoicechat/tts/base.py</code> <pre><code>def __init__(self, sample_rate: int, player=sd, wait=True, logger=None):\n    \"\"\"\n    Initializes the BaseMouth class.\n\n    :param sample_rate: The sample rate of the audio.\n    :param player: The audio player object. Defaults to sounddevice.\n    :param wait: Whether to wait for the audio to finish playing. Defaults to True.\n    \"\"\"\n    self.sample_rate = sample_rate\n    self.interrupted = \"\"\n    self.player = player\n    self.seg = pysbd.Segmenter(language=\"en\", clean=True)\n    self.wait = wait\n    self.logger = logger\n</code></pre>"},{"location":"base_classes/#tts.base.BaseMouth.sample_rate","title":"sample_rate  <code>instance-attribute</code>","text":"<pre><code>sample_rate = sample_rate\n</code></pre>"},{"location":"base_classes/#tts.base.BaseMouth.interrupted","title":"interrupted  <code>instance-attribute</code>","text":"<pre><code>interrupted = ''\n</code></pre>"},{"location":"base_classes/#tts.base.BaseMouth.player","title":"player  <code>instance-attribute</code>","text":"<pre><code>player = player\n</code></pre>"},{"location":"base_classes/#tts.base.BaseMouth.seg","title":"seg  <code>instance-attribute</code>","text":"<pre><code>seg = Segmenter(language='en', clean=True)\n</code></pre>"},{"location":"base_classes/#tts.base.BaseMouth.wait","title":"wait  <code>instance-attribute</code>","text":"<pre><code>wait = wait\n</code></pre>"},{"location":"base_classes/#tts.base.BaseMouth.logger","title":"logger  <code>instance-attribute</code>","text":"<pre><code>logger = logger\n</code></pre>"},{"location":"base_classes/#tts.base.BaseMouth.run_tts","title":"run_tts","text":"<pre><code>run_tts(text: str) -&gt; np.ndarray\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The text to synthesize speech for</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>audio numpy array for sounddevice</p> Source code in <code>openvoicechat/tts/base.py</code> <pre><code>def run_tts(self, text: str) -&gt; np.ndarray:\n    \"\"\"\n    :param text: The text to synthesize speech for\n    :return: audio numpy array for sounddevice\n    \"\"\"\n    raise NotImplementedError(\"This method should be implemented by the subclass\")\n</code></pre>"},{"location":"base_classes/#tts.base.BaseMouth.say_text","title":"say_text","text":"<pre><code>say_text(text: str)\n</code></pre> <p>calls run_tts and plays the audio using the player.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The text to synthesize speech for</p> required Source code in <code>openvoicechat/tts/base.py</code> <pre><code>def say_text(self, text: str):\n    \"\"\"\n    calls run_tts and plays the audio using the player.\n    :param text: The text to synthesize speech for\n    \"\"\"\n    output = self.run_tts(text)\n    self.player.play(output, samplerate=self.sample_rate)\n    self.player.wait()\n</code></pre>"},{"location":"base_classes/#tts.base.BaseMouth.say","title":"say","text":"<pre><code>say(audio_queue: queue.Queue, listen_interruption_func: Callable)\n</code></pre> <p>Plays the audios in the queue using the player. Stops if interruption occurred.</p> <p>Parameters:</p> Name Type Description Default <code>audio_queue</code> <code>Queue</code> <p>The queue where the audio is stored for it to be played</p> required <code>listen_interruption_func</code> <code>Callable</code> <p>callable function from the ear class.</p> required Source code in <code>openvoicechat/tts/base.py</code> <pre><code>def say(self, audio_queue: queue.Queue, listen_interruption_func: Callable):\n    \"\"\"\n    Plays the audios in the queue using the player. Stops if interruption occurred.\n    :param audio_queue: The queue where the audio is stored for it to be played\n    :param listen_interruption_func: callable function from the ear class.\n    \"\"\"\n    self.interrupted = \"\"\n    while True:\n        output, text = audio_queue.get()\n        if output is None:\n            self.player.wait()  # wait for the last audio to finish\n            break\n        # get the duration of audio\n        duration = len(output) / self.sample_rate\n        self._log_event(\"playing audio\", \"TTS\", f\"{duration} seconds\")\n        self.player.play(output, samplerate=self.sample_rate)\n        interruption = listen_interruption_func(duration)\n        if interruption:\n            self._log_event(\"audio interrupted\", f\"TTS\")\n            self.player.stop()\n            self.interrupted = (interruption, text)\n            break\n        else:\n            if self.wait:\n                self.player.wait()  # No need for wait here\n</code></pre>"},{"location":"base_classes/#tts.base.BaseMouth.say_multiple","title":"say_multiple","text":"<pre><code>say_multiple(text: str, listen_interruption_func: Callable)\n</code></pre> <p>Splits the text into sentences. Then plays the sentences one by one using run_tts() and say()</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>Input text to synthesize</p> required <code>listen_interruption_func</code> <code>Callable</code> <p>callable function from the ear class</p> required Source code in <code>openvoicechat/tts/base.py</code> <pre><code>def say_multiple(self, text: str, listen_interruption_func: Callable):\n    \"\"\"\n    Splits the text into sentences. Then plays the sentences one by one\n    using run_tts() and say()\n\n    :param text: Input text to synthesize\n    :param listen_interruption_func: callable function from the ear class\n    \"\"\"\n    sentences = self.seg.segment(text)\n    print(sentences)\n    audio_queue = queue.Queue()\n    say_thread = threading.Thread(\n        target=self.say, args=(audio_queue, listen_interruption_func)\n    )\n    say_thread.start()\n    for sentence in sentences:\n        output = self.run_tts(sentence)\n        audio_queue.put((output, sentence))\n        if self.interrupted:\n            break\n    audio_queue.put((None, \"\"))\n    say_thread.join()\n</code></pre>"},{"location":"base_classes/#tts.base.BaseMouth._handle_interruption","title":"_handle_interruption","text":"<pre><code>_handle_interruption(responses_list, interrupt_queue)\n</code></pre> Source code in <code>openvoicechat/tts/base.py</code> <pre><code>def _handle_interruption(self, responses_list, interrupt_queue):\n    interrupt_transcription, interrupt_text = self.interrupted\n    self._log_event(\"interruption detected\", \"TTS\", interrupt_transcription)\n    idx = responses_list.index(interrupt_text)\n    assert (\n        idx != -1\n    ), \"Interrupted text not found in responses list. This should not happen. Raise an issue.\"\n    responses_list = responses_list[:idx] + [\"...\"]\n    interrupt_queue.put(interrupt_transcription)\n    return responses_list\n</code></pre>"},{"location":"base_classes/#tts.base.BaseMouth._get_all_text","title":"_get_all_text","text":"<pre><code>_get_all_text(text_queue)\n</code></pre> Source code in <code>openvoicechat/tts/base.py</code> <pre><code>def _get_all_text(self, text_queue):\n    text = text_queue.get()\n    while not text_queue.empty():\n        new_text = text_queue.get()\n        if new_text is not None:\n            text += new_text\n        else:\n            text_queue.put(None)\n            break\n    return text\n</code></pre>"},{"location":"base_classes/#tts.base.BaseMouth._log_event","title":"_log_event","text":"<pre><code>_log_event(event: str, details: str, further: str = '')\n</code></pre> Source code in <code>openvoicechat/tts/base.py</code> <pre><code>def _log_event(self, event: str, details: str, further: str = \"\"):\n    if self.logger:\n        self.logger.info(\n            event, extra={\"details\": details, \"further\": f'\"{further}\"'}\n        )\n</code></pre>"},{"location":"base_classes/#tts.base.BaseMouth.say_multiple_stream","title":"say_multiple_stream","text":"<pre><code>say_multiple_stream(text_queue: queue.Queue, listen_interruption_func: Callable, interrupt_queue: queue.Queue, audio_queue: queue.Queue = None)\n</code></pre> <p>Receives text from the text_queue. As soon as a sentence is made run_tts is called to synthesize its speech and sent to the audio_queue for it to be played.</p> <p>Parameters:</p> Name Type Description Default <code>text_queue</code> <code>Queue</code> <p>The queue where the llm adds the predicted tokens</p> required <code>listen_interruption_func</code> <code>Callable</code> <p>callable function from the ear class</p> required <code>interrupt_queue</code> <code>Queue</code> <p>The queue where True is put when interruption occurred.</p> required <code>audio_queue</code> <code>Queue</code> <p>The queue where the audio to be played is placed</p> <code>None</code> Source code in <code>openvoicechat/tts/base.py</code> <pre><code>def say_multiple_stream(\n    self,\n    text_queue: queue.Queue,\n    listen_interruption_func: Callable,\n    interrupt_queue: queue.Queue,\n    audio_queue: queue.Queue = None,\n):\n    \"\"\"\n    Receives text from the text_queue. As soon as a sentence is made run_tts is called to\n    synthesize its speech and sent to the audio_queue for it to be played.\n\n    :param text_queue: The queue where the llm adds the predicted tokens\n    :param listen_interruption_func: callable function from the ear class\n    :param interrupt_queue: The queue where True is put when interruption occurred.\n    :param audio_queue: The queue where the audio to be played is placed\n\n    \"\"\"\n    response = \"\"\n    all_response = []\n    interrupt_text_list = []\n\n    if audio_queue is None:\n        audio_queue = queue.Queue()\n    say_thread = threading.Thread(\n        target=self.say, args=(audio_queue, listen_interruption_func)\n    )\n    self._log_event(\"audio play thread started\", \"TTS\")\n    say_thread.start()\n    text = \"\"\n\n    while text is not None:\n        self._log_event(\"getting all text\", \"TTS\")\n        text = self._get_all_text(text_queue)\n        self._log_event(\"all text received\", \"TTS\")\n\n        if text is None:\n            self._log_event(\"Stream ended\", \"TTS\")\n            sentence = response\n        else:\n            response += text\n            self._log_event(\"segmenting text\", \"TTS\", response)\n            sentences = self.seg.segment(response)\n            # if there are multiple sentences we split and play the first one\n            if len(sentences) &gt; 1:\n                self._log_event(\"multiple sentences detected\", \"TTS\")\n                sentence = sentences[0]\n                response = \" \".join([s for s in sentences[1:] if s != \".\"])\n            else:\n                self._log_event(\"single sentence detected\", \"TTS\")\n                continue\n\n        if sentence.strip() != \"\":\n            self._log_event(\"cleaning sentence\", \"TTS\")\n            clean_sentence = remove_words_in_brackets_and_spaces(sentence).strip()\n            if (\n                clean_sentence.strip() != \"\"\n            ):  # sentence only contains words in brackets\n                self._log_event(\"running tts\", \"TTS\", clean_sentence)\n                output = self.run_tts(clean_sentence)\n                self._log_event(\"tts output received\", \"TTS\")\n                audio_queue.put((output, clean_sentence))\n                interrupt_text_list.append(clean_sentence)\n            all_response.append(sentence)\n        # if interruption occurred, handle it\n        if self.interrupted:\n            all_response = self._handle_interruption(\n                interrupt_text_list, interrupt_queue\n            )\n            self.interrupted = \"\"\n            break\n\n    audio_queue.put((None, \"\"))\n\n    say_thread.join()\n    self._log_event(\"audio play thread ended\", \"TTS\")\n    if self.interrupted:\n        all_response = self._handle_interruption(\n            interrupt_text_list, interrupt_queue\n        )\n    text_queue.queue.clear()\n    text_queue.put(\" \".join(all_response))\n</code></pre>"},{"location":"docs/","title":"OpenVoiceChat","text":"<p>OpenVoiceChat is an opensource library that allows you to have natural voice conversations with your LLM agent.</p> <p>If you plan on making an LLM agent and want to have your users be able to talk to it like a person (low latency, handles interruptions), this library is for you</p> <p>It aims to be the opensource, highly extensible and easy to use alternative to the proprietary solutions.</p>"},{"location":"llm_classes/","title":"LLM classes","text":""},{"location":"llm_classes/#llm.llm_gpt.Chatbot_gpt","title":"llm.llm_gpt.Chatbot_gpt","text":"<pre><code>Chatbot_gpt(sys_prompt='', Model='gpt-4o-mini', api_key='', tools=None, tool_choice=NOT_GIVEN, tool_utterances=None, functions=None, logger=None)\n</code></pre> <p>               Bases: <code>BaseChatbot</code></p> <p>Methods:</p> Name Description <code>run</code> <code>post_process</code> <p>Attributes:</p> Name Type Description <code>MODEL</code> <code>client</code> <code>messages</code> <code>tools</code> <code>tool_choice</code> <code>tool_utterances</code> <code>functions</code> Source code in <code>openvoicechat/llm/llm_gpt.py</code> <pre><code>def __init__(\n    self,\n    sys_prompt=\"\",\n    Model=\"gpt-4o-mini\",\n    api_key=\"\",\n    tools=None,\n    tool_choice=NOT_GIVEN,\n    tool_utterances=None,\n    functions=None,\n    logger=None,\n):\n    super().__init__(logger=logger)\n\n    if tools is None:\n        tools = NOT_GIVEN\n    if tool_utterances is None:\n        tool_utterances = {}\n    if functions is None:\n        self.functions = {}\n\n    from openai import OpenAI\n    from dotenv import load_dotenv\n\n    if api_key == \"\":\n        load_dotenv()\n        api_key = os.getenv(\"OPENAI_API_KEY\")\n\n    self.MODEL = Model\n    self.client = OpenAI(api_key=api_key)\n    self.messages = []\n    self.messages.append({\"role\": \"system\", \"content\": sys_prompt})\n    self.tools = tools\n    self.tool_choice = tool_choice\n    self.tool_utterances = tool_utterances\n    self.functions = functions\n</code></pre>"},{"location":"llm_classes/#llm.llm_gpt.Chatbot_gpt.MODEL","title":"MODEL  <code>instance-attribute</code>","text":"<pre><code>MODEL = Model\n</code></pre>"},{"location":"llm_classes/#llm.llm_gpt.Chatbot_gpt.client","title":"client  <code>instance-attribute</code>","text":"<pre><code>client = OpenAI(api_key=api_key)\n</code></pre>"},{"location":"llm_classes/#llm.llm_gpt.Chatbot_gpt.messages","title":"messages  <code>instance-attribute</code>","text":"<pre><code>messages = []\n</code></pre>"},{"location":"llm_classes/#llm.llm_gpt.Chatbot_gpt.tools","title":"tools  <code>instance-attribute</code>","text":"<pre><code>tools = tools\n</code></pre>"},{"location":"llm_classes/#llm.llm_gpt.Chatbot_gpt.tool_choice","title":"tool_choice  <code>instance-attribute</code>","text":"<pre><code>tool_choice = tool_choice\n</code></pre>"},{"location":"llm_classes/#llm.llm_gpt.Chatbot_gpt.tool_utterances","title":"tool_utterances  <code>instance-attribute</code>","text":"<pre><code>tool_utterances = tool_utterances\n</code></pre>"},{"location":"llm_classes/#llm.llm_gpt.Chatbot_gpt.functions","title":"functions  <code>instance-attribute</code>","text":"<pre><code>functions = functions\n</code></pre>"},{"location":"llm_classes/#llm.llm_gpt.Chatbot_gpt.run","title":"run","text":"<pre><code>run(input_text)\n</code></pre> Source code in <code>openvoicechat/llm/llm_gpt.py</code> <pre><code>def run(self, input_text):\n    self.messages.append({\"role\": \"user\", \"content\": input_text})\n    finished = False\n    while not finished:\n\n        func_call = dict()\n        function_call_detected = False\n\n        stream = self.client.chat.completions.create(\n            model=self.MODEL,\n            messages=self.messages,\n            stream=True,\n            tools=self.tools,\n            tool_choice=self.tool_choice,\n        )\n\n        for chunk in stream:\n            finish_reason = chunk.choices[0].finish_reason\n            if chunk.choices[0].delta.tool_calls is not None:\n                function_call_detected = True\n                tool_call = chunk.choices[0].delta.tool_calls[0]\n                if tool_call.function.name:\n                    func_call[\"name\"] = tool_call.function.name\n                    func_call[\"id\"] = tool_call.id\n                    func_call[\"arguments\"] = \"\"\n                    # Choose a utterance for the tool at random and output it for the tts\n                    yield random.choice(\n                        self.tool_utterances[func_call[\"name\"]]\n                    ) + \" . \"  # the period is to make\n                    # it say immediately\n                if tool_call.function.arguments:\n                    func_call[\"arguments\"] += tool_call.function.arguments\n            if function_call_detected and finish_reason == \"tool_calls\":\n                self.messages.append(\n                    {\n                        \"role\": \"assistant\",\n                        \"content\": None,\n                        \"tool_calls\": [\n                            {\n                                \"id\": func_call[\"id\"],\n                                \"type\": \"function\",\n                                \"function\": {\n                                    \"name\": func_call[\"name\"],\n                                    \"arguments\": func_call[\"arguments\"],\n                                },\n                            }\n                        ],\n                    }\n                )\n                # run the function\n                function_response = self.functions[func_call[\"name\"]](\n                    **json.loads(func_call[\"arguments\"])\n                )\n                self.messages.append(\n                    {\n                        \"tool_call_id\": func_call[\"id\"],\n                        \"role\": \"tool\",\n                        \"name\": func_call[\"name\"],\n                        \"content\": function_response,\n                    }\n                )\n            if chunk.choices[0].delta.content is not None:\n                yield chunk.choices[0].delta.content\n            if finish_reason == \"stop\":\n                finished = True\n</code></pre>"},{"location":"llm_classes/#llm.llm_gpt.Chatbot_gpt.post_process","title":"post_process","text":"<pre><code>post_process(response)\n</code></pre> Source code in <code>openvoicechat/llm/llm_gpt.py</code> <pre><code>def post_process(self, response):\n    # remove the tool utterances from the response\n    # for tool in self.tool_utterances:\n    #     response = response.replace(self.tool_utterances[tool], '')\n    self.messages.append({\"role\": \"assistant\", \"content\": response})\n    return response\n</code></pre>"},{"location":"llm_classes/#llm.llm_llama.Chatbot_llama","title":"llm.llm_llama.Chatbot_llama","text":"<pre><code>Chatbot_llama(model_path='models/llama-2-7b-chat.Q4_K_M.gguf', device='cuda', sys_prompt='', chat_format=None, temperature=0.7)\n</code></pre> <p>               Bases: <code>BaseChatbot</code></p> <p>Methods:</p> Name Description <code>run</code> <code>post_process</code> <p>Attributes:</p> Name Type Description <code>model</code> <code>messages</code> <code>temperature</code> Source code in <code>openvoicechat/llm/llm_llama.py</code> <pre><code>def __init__(\n    self,\n    model_path=\"models/llama-2-7b-chat.Q4_K_M.gguf\",\n    device=\"cuda\",\n    sys_prompt=\"\",\n    chat_format=None,\n    temperature=0.7,\n):\n\n    from llama_cpp import Llama\n\n    self.model = Llama(\n        model_path=model_path,\n        n_ctx=4096,\n        n_gpu_layers=-1 if device == \"cuda\" else 0,\n        verbose=False,\n        chat_format=chat_format,\n    )\n    self.messages = [{\"role\": \"system\", \"content\": sys_prompt}]\n    self.temperature = temperature\n</code></pre>"},{"location":"llm_classes/#llm.llm_llama.Chatbot_llama.model","title":"model  <code>instance-attribute</code>","text":"<pre><code>model = Llama(model_path=model_path, n_ctx=4096, n_gpu_layers=-1 if device == 'cuda' else 0, verbose=False, chat_format=chat_format)\n</code></pre>"},{"location":"llm_classes/#llm.llm_llama.Chatbot_llama.messages","title":"messages  <code>instance-attribute</code>","text":"<pre><code>messages = [{'role': 'system', 'content': sys_prompt}]\n</code></pre>"},{"location":"llm_classes/#llm.llm_llama.Chatbot_llama.temperature","title":"temperature  <code>instance-attribute</code>","text":"<pre><code>temperature = temperature\n</code></pre>"},{"location":"llm_classes/#llm.llm_llama.Chatbot_llama.run","title":"run","text":"<pre><code>run(input_text)\n</code></pre> Source code in <code>openvoicechat/llm/llm_llama.py</code> <pre><code>def run(self, input_text):\n    self.messages.append({\"role\": \"user\", \"content\": input_text})\n    out = self.model.create_chat_completion(\n        self.messages, stream=True, temperature=self.temperature\n    )\n    response_text = \"\"\n    for o in out:\n        if \"content\" in o[\"choices\"][0][\"delta\"].keys():\n            text = o[\"choices\"][0][\"delta\"][\"content\"]\n            response_text += text\n            yield text\n        if o[\"choices\"][0][\"finish_reason\"] is not None:\n            break\n</code></pre>"},{"location":"llm_classes/#llm.llm_llama.Chatbot_llama.post_process","title":"post_process","text":"<pre><code>post_process(response)\n</code></pre> Source code in <code>openvoicechat/llm/llm_llama.py</code> <pre><code>def post_process(self, response):\n    self.messages.append({\"role\": \"assistant\", \"content\": response})\n    return response\n</code></pre>"},{"location":"llm_classes/#llm.prompts","title":"llm.prompts","text":"<p>Attributes:</p> Name Type Description <code>Male_Voice</code> <code>Female_Voice</code> <code>llama_sales</code> <code>call_pre_prompt</code> <code>advisor_pre_prompt</code> <code>sales_pre_prompt</code>"},{"location":"llm_classes/#llm.prompts.Male_Voice","title":"Male_Voice  <code>module-attribute</code>","text":"<pre><code>Male_Voice = '\\nA male speaker with a low-pitched voice delivering his words at a fast pace in a small, confined space with a very clear audio and an animated tone.\\n'\n</code></pre>"},{"location":"llm_classes/#llm.prompts.Female_Voice","title":"Female_Voice  <code>module-attribute</code>","text":"<pre><code>Female_Voice = '\\nA female speaker with a slightly low-pitched, quite monotone voice delivers her words at a slightly faster-than-average pace in a confined space with very clear audio.\\n'\n</code></pre>"},{"location":"llm_classes/#llm.prompts.llama_sales","title":"llama_sales  <code>module-attribute</code>","text":"<pre><code>llama_sales = \"You are a call sales agent for apple. Your name is John.\\nYou are friendly, helpful, sometimes a little funny and very knowledgeable about Apple products.\\nYou are a good listener and can understand customers' needs.\\nYou will be on the phone with customers for a long time, so you need to be patient and polite.\\nKeep your responses short and to the point, do not include any emojis and actions like *smiles* or *nods* in your responses.\\nOutput [END] when the conversation is over.\"\n</code></pre>"},{"location":"llm_classes/#llm.prompts.call_pre_prompt","title":"call_pre_prompt  <code>module-attribute</code>","text":"<pre><code>call_pre_prompt = \"\\nJOHN is a call center agent for apple. JOHN is a good salesman and he is very good at his job. JOHN closes almost every \\ndeal he gets. JOHN can sell anything, he is that good.\\n\\n[START]\\n[JOHN] Good morning! Thank you for calling Apple. My name is John, how can I assist you today?\\n[USER] Hi, John. I'm interested in purchasing a new iPhone, but I'm not sure which model would be the best fit for me.\\n[JOHN] Absolutely! I'd be happy to help you find the perfect iPhone. May I ask what features or specifications you're looking for in your new phone?\\n[USER] Well, I'm a photographer, so having a great camera is essential for me. I also need a phone with good battery life and ample storage for my photos.\\n[JOHN] That's fantastic! The latest iPhone models offer impressive camera capabilities, long battery life, and various storage options. Based on your needs, I would recommend the iPhone 13 Pro. It has a triple-camera system, exceptional battery performance, and storage options up to 1TB.\\n[USER] Oh, that sounds interesting. Could you tell me more about its camera features?\\n[JOHN] Certainly! With the iPhone 13 Pro, you'll capture stunning photos with its advanced camera system, ensuring your photography stands out. Plus, its longer battery life will keep you going throughout your busy days without worrying about recharging frequently. And with the ample storage, you'll have plenty of space for all your photos and more.\\n[USER] That sounds promising. But I'm concerned about the price. Are there any deals available?\\n[JOHN] Absolutely, we do have promotions running currently. We offer trade-in options for your current device, which can significantly reduce the cost. Additionally, we have financing options that can spread the payments over time, making it more manageable.\\n[USER] That's good to know. Can you explain more about the trade-in process?\\n[JOHN] Of course! Based on our discussion, the iPhone 13 Pro seems like the perfect fit for your needs. Shall we proceed with the purchase? I can assist you with the order and walk you through the process step-by-step.\\n[USER] Yes, let's proceed with it.\\n[JOHN] Wonderful! I'll guide you through the checkout process. Is there anything else I can assist you with today?\\n[USER] Thank you, John. I appreciate your help.\\n[JOHN] Thank you for choosing Apple. You've made an excellent choice with the iPhone 13 Pro. If you have any further questions or need assistance in the future, feel free to reach out. Have a fantastic day!\\n[END]\\n[START]\\n[JOHN] Good morning! Thank you for calling Apple. My name is John, how can I assist you today?\\n[USER] Hi, John. I've been eyeing the new MacBook Pro, but I'm concerned about the price.\\n[JOHN] I completely understand. The MacBook Pro is an incredible device. What features of the MacBook Pro are you particularly interested in?\\n[USER] I need a powerful laptop for my graphic design work, and I've heard great things about the performance of the MacBook Pro.\\n[JOHN] Absolutely, the MacBook Pro is indeed an excellent choice for graphic design. Its high-performance processors and stunning display make it ideal for creative work. However, I understand that affordability is a concern for you. Have you considered any specific model or configuration?\\n[USER] I was looking at the latest model with higher specs, but it's beyond my budget at the moment.\\n[JOHN] I see. Well, we have various models and configurations available, and some older models might still meet your needs at a more affordable price point. Additionally, we offer financing options that could help make it more manageable for you. Would you like me to explore those alternatives with you?\\n[USER] I'm not sure if financing would work for me right now. I'm trying to manage my expenses carefully.\\n[JOHN] I completely understand the importance of managing expenses. Another option could be our certified refurbished MacBook Pros. They go through a rigorous refurbishment process, ensuring quality, and come at a more budget-friendly price. They also include the same warranty as new products. Would you like more information about our refurbished options?\\n[USER] That sounds interesting. Could you provide more details about the refurbished models available?\\n[JOHN] Absolutely! Our refurbished MacBook Pros undergo comprehensive testing, replacing any faulty parts and ensuring they meet Apple's high standards. They come with the same warranty as new devices, and often, customers find them to be a great value for their money.\\n[USER] That might be a good option for me. Can I take some time to think about it and call back later?\\n[JOHN] Absolutely, take your time to consider it. Whenever you're ready, feel free to reach out to us. We're here to assist you further and answer any questions you might have. Thank you for considering Apple, and I hope we can find a solution that fits your needs and budget.\\n[END]\\n[START]\\n[JOHN] Good morning! Thank you for calling Apple. My name is John, how can I assist you today?\\n[USER] Hi, John. I'm interested in purchasing some Apple products, but I'm not sure about the prices.\\n[JOHN] Absolutely! I'd be happy to provide you with pricing information. Which Apple products are you considering?\\n[USER] I'm interested in buying an iPad for my daughter and maybe a new MacBook for myself.\\n[JOHN] Great choices! Our current iPad lineup offers different models at various price points. The iPad starts at 329 dollars, while the iPad Pro starts at 799 dollars. As for the MacBook, our latest MacBook Air starts at $999, and the MacBook Pro starts at 1299 dollars.\\n[USER] Hmm, those prices are a bit higher than I anticipated. Are there any discounts or promotions available?\\n[JOHN] Absolutely, we do have ongoing promotions. Additionally, we've recently introduced the Apple Certified Refurbished program, where you can get quality Apple products at a reduced price with the same warranty as new devices. Also, have you considered the new MacBook Air with the M1 chip? It's a powerful and energy-efficient option that could meet your needs at a competitive price.\\n[USER] I haven't heard about that. Can you tell me more about the new MacBook Air?\\n[JOHN] Sure! The new MacBook Air with the M1 chip delivers incredible performance, improved battery life, and remarkable speed compared to previous models. It's designed to handle everyday tasks and demanding workloads efficiently, all while maintaining its sleek and lightweight design.\\n[USER] That sounds interesting! Is it priced similarly to the other MacBook Air models?\\n[JOHN] Yes, the new MacBook Air with the M1 chip starts at 999 dollars, which is in line with our standard MacBook Air pricing. However, its enhanced performance and efficiency make it a great value for its price.\\n[USER] I'll definitely consider that option. Do you have any ongoing deals for it?\\n[JOHN] Yes, we currently have a promotion offering a discount on select accessories when purchasing the new MacBook Air. Additionally, we offer trade-in options and financing that can help lower the overall cost. Would you like more details about these offers?\\n[USER] Yes, please. That sounds promising.\\n[JOHN] Absolutely! I can provide you with more information and assist you further in exploring these offers. Shall we proceed?\\n[USER] Yes, please. That would be great.\\n[JOHN] Wonderful! I'll guide you through the available deals and options. Is there anything else I can assist you with today?\\n[USER] That's all for now. Thank you for your help, John.\\n[JOHN] Thank you for considering Apple. I'm here to assist you at any time. Have a fantastic day!\\n[END]\\n[START]\\n[JOHN] Good afternoon! Thank you for calling Apple. My name is John, how can I assist you today?\\n[USER] \"\n</code></pre>"},{"location":"llm_classes/#llm.prompts.advisor_pre_prompt","title":"advisor_pre_prompt  <code>module-attribute</code>","text":"<pre><code>advisor_pre_prompt = \"\\nJOHN is the best advisor in town. He gives the best advice to his users. John does not say bad words, John is a good buy,\\n\\n[USER] I'm at a crossroads in my life and I'm not sure which direction to take. Can you give me some guidance?\\n[JOHN] Of course, I'm here to help. Can you tell me a bit more about what's been on your mind?\\n[USER] I recently finished college and I'm struggling to figure out what to do next. I feel like I don't have a clear sense of direction.\\n[JOHN] That's understandable. What are some interests or passions you have that could potentially guide you in a certain direction?\\n[USER] I enjoy creative writing and I'm good at conducting research and analyzing data.\\n[JOHN] That's great to hear. Have you considered a career in writing or journalism? Those fields could allow you to utilize your research and analysis skills while also allowing for creativity. It might also be helpful to connect with professionals in those fields or pursue internships to gain some practical experience.\\n[USER] I'm not sure if I want to jump right into a career. I feel like I need to take some time to explore my options.\\n[JOHN] That's understandable. There's no rush to make a decision right away. Have you thought about volunteering or taking a gap year to gain some new experiences and gain some clarity on your interests?\\n[USER] I haven't really considered that, but it's a good idea.\\n[JOHN] It can be a great way to gain new skills and perspectives, and to figure out what you're really passionate about. Another option could be to take online courses or attend workshops to develop new skills and gain some clarity on your interests.\\n[USER] I'm worried about making the wrong choice and regretting it later.\\n[JOHN] It's natural to have concerns about making the right decision. However, it's important to remember that every decision is an opportunity for growth and learning. There may be some bumps along the way, but that's how we learn and improve. Just stay open to new opportunities and be willing to take some risks.\\n[USER] I'm also worried about financial stability.\\n[JOHN] That's a valid concern. It's important to consider your financial needs and goals when making career decisions. Have you looked into potential salaries or job growth in the fields you're interested in?\\n[USER] Not really, I'm not sure where to start.\\n[JOHN] That's okay. You can start by researching job listings and salary information online. You can also reach out to professionals in those fields to gain some insights into the job market and potential career paths.\\n[USER] I'm also worried about disappointing my family.\\n[JOHN] It's understandable to want to make your loved ones proud, but ultimately, you have to make the decision that feels right for you. It may be helpful to have an open and honest conversation with your family about your concerns and goals. They may be more supportive than you think.\\n[USER] I'm also feeling pressure to be successful.\\n[JOHN] It's important to remember that success means different things to different people. Try to focus on what success means to you, and don't worry too much about external expectations. It's also important to remember that success is not a linear path, and there may be some setbacks along the way.\\n[USER] I'm feeling overwhelmed and anxious about all of this.\\n[JOHN] It's natural to feel overwhelmed at times, but try to take things one step at a time. Focus on small actions you can take to move towards your goals. It may also be helpful to practice self-care activities like exercise,\\n[USER] \"\n</code></pre>"},{"location":"llm_classes/#llm.prompts.sales_pre_prompt","title":"sales_pre_prompt  <code>module-attribute</code>","text":"<pre><code>sales_pre_prompt = \"\\nJOHN is a saleman for Fakhir's tea. JOHN has been selling the tea his entire life. JOHN is a great tea salesman.\\n\\n[USER] Hey, how's it going?\\n[JOHN] Good, good. How about you? Say, have you tried any good drinks lately?\\n[USER] Not really, just my usual coffee and water.\\n[JOHN] Ah, well let me tell you, I've been really getting into tea lately. Specifically, Fakhir's Tea. Have you heard of it?\\n[USER] No, I don't think so. What's so great about it?\\n[JOHN] Oh, it's just amazing. Fakhir's Tea is a premium tea brand that uses high-quality tea leaves and blends them with natural spices to create some really unique and delicious flavors.\\n[USER] That sounds interesting. Where can I find it?\\n[JOHN] You can find it at most grocery stores and online retailers, but I've found that ordering directly from their website gets you the best deals and the most variety. Plus, their customer service is top-notch.\\n[USER] What flavors do they have?\\n[JOHN] They have a ton of different blends, from classic black teas to more exotic flavors like cardamom and saffron. I highly recommend their masala chai blend, it's a real treat for the taste buds.\\n[USER] I'll have to check it out. Thanks for the recommendation.\\n[JOHN] No problem at all, happy to help. Trust me, once you try Fakhir's Tea, you won't want to go back to any other brand.\\n[USER] Hey, how's it going?\\n[JOHN] Great, just enjoying my regular tea. What are you up to?\\n[USER] Not really, just trying to stay busy with work and everything. How about you?\\n[JOHN] Same here, just staying busy. Hey, have you ever tried Fakhir's Tea?\\n[USER] No, I don't think so. What's that?\\n[JOHN] It's this amazing brand of tea that I recently discovered. They use only the highest quality tea leaves and blend them with natural spices for some really unique and delicious flavors.\\n[USER] That does sound interesting. What kind of flavors do they have?\\n[JOHN] Oh, they have a ton of flavors to choose from. From classic black tea to more exotic blends like cardamom and saffron. You really have to try it to appreciate it.\\n[USER] Where can I find it?\\n[JOHN] You can find it at most grocery stores and online retailers, but I highly recommend ordering directly from their website. They have some really great deals and it's super convenient.\\n[USER] Alright, thanks for the recommendation. I'll have to check it out.\\n[JOHN] No problem at all. Trust me, once you try Fakhir's Tea, you'll never want to go back to regular old tea again.\\n[USER] \"\n</code></pre>"},{"location":"stt_classes/","title":"Speech to text classes","text":""},{"location":"stt_classes/#stt.stt_deepgram.Ear_deepgram","title":"stt.stt_deepgram.Ear_deepgram","text":"<pre><code>Ear_deepgram(silence_seconds=2, api_key='', listener=None, logger=None)\n</code></pre> <p>               Bases: <code>BaseEar</code></p> <p>Methods:</p> Name Description <code>transcribe_stream</code> <p>Attributes:</p> Name Type Description <code>api_key</code> Source code in <code>openvoicechat/stt/stt_deepgram.py</code> <pre><code>def __init__(self, silence_seconds=2, api_key=\"\", listener=None, logger=None):\n    super().__init__(silence_seconds, stream=True, listener=listener, logger=logger)\n    self.api_key = api_key\n</code></pre>"},{"location":"stt_classes/#stt.stt_deepgram.Ear_deepgram.api_key","title":"api_key  <code>instance-attribute</code>","text":"<pre><code>api_key = api_key\n</code></pre>"},{"location":"stt_classes/#stt.stt_deepgram.Ear_deepgram.transcribe_stream","title":"transcribe_stream","text":"<pre><code>transcribe_stream(audio_queue, transcription_queue)\n</code></pre> Source code in <code>openvoicechat/stt/stt_deepgram.py</code> <pre><code>def transcribe_stream(self, audio_queue, transcription_queue):\n    extra_headers = {\"Authorization\": \"token \" + self.api_key}\n\n    async def f():\n        async with websockets.connect(\n            \"wss://api.deepgram.com/v1/listen?encoding=linear16&amp;sample_rate=16000\"\n            \"&amp;channels=1&amp;model=nova-2\",\n            extra_headers=extra_headers,\n        ) as ws:\n\n            async def sender(ws):  # sends audio to websocket\n                try:\n                    while True:\n                        data = audio_queue.get()\n                        if data is None:\n                            await ws.send(json.dumps({\"type\": \"CloseStream\"}))\n                            break\n                        await ws.send(data)\n                except Exception as e:\n                    print(\"Error while sending: \", str(e))\n                    raise\n\n            async def receiver(ws):\n                async for msg in ws:\n                    msg = json.loads(msg)\n                    if \"channel\" not in msg:\n                        transcription_queue.put(None)\n                        break\n                    transcript = msg[\"channel\"][\"alternatives\"][0][\"transcript\"]\n\n                    if transcript:\n                        transcription_queue.put(transcript)\n\n            await asyncio.gather(sender(ws), receiver(ws))\n\n    asyncio.run(f())\n</code></pre>"},{"location":"stt_classes/#stt.stt_hf.Ear_hf","title":"stt.stt_hf.Ear_hf","text":"<pre><code>Ear_hf(model_id='openai/whisper-base.en', device='cpu', silence_seconds=2, generate_kwargs=None, listener=None, listen_interruptions=True, logger=None)\n</code></pre> <p>               Bases: <code>BaseEar</code></p> <p>Methods:</p> Name Description <code>transcribe</code> <p>Attributes:</p> Name Type Description <code>pipe</code> <code>device</code> <code>generate_kwargs</code> Source code in <code>openvoicechat/stt/stt_hf.py</code> <pre><code>def __init__(\n    self,\n    model_id=\"openai/whisper-base.en\",\n    device=\"cpu\",\n    silence_seconds=2,\n    generate_kwargs=None,\n    listener=None,\n    listen_interruptions=True,\n    logger=None,\n):\n    super().__init__(\n        silence_seconds,\n        listener=listener,\n        listen_interruptions=listen_interruptions,\n        logger=logger,\n    )\n    from transformers import pipeline\n\n    self.pipe = pipeline(\n        \"automatic-speech-recognition\", model=model_id, device=device\n    )\n    self.device = device\n    self.generate_kwargs = generate_kwargs\n</code></pre>"},{"location":"stt_classes/#stt.stt_hf.Ear_hf.pipe","title":"pipe  <code>instance-attribute</code>","text":"<pre><code>pipe = pipeline('automatic-speech-recognition', model=model_id, device=device)\n</code></pre>"},{"location":"stt_classes/#stt.stt_hf.Ear_hf.device","title":"device  <code>instance-attribute</code>","text":"<pre><code>device = device\n</code></pre>"},{"location":"stt_classes/#stt.stt_hf.Ear_hf.generate_kwargs","title":"generate_kwargs  <code>instance-attribute</code>","text":"<pre><code>generate_kwargs = generate_kwargs\n</code></pre>"},{"location":"stt_classes/#stt.stt_hf.Ear_hf.transcribe","title":"transcribe","text":"<pre><code>transcribe(audio)\n</code></pre> Source code in <code>openvoicechat/stt/stt_hf.py</code> <pre><code>def transcribe(self, audio):\n    from torch import no_grad\n\n    with no_grad():\n        transcription = self.pipe(audio, generate_kwargs=self.generate_kwargs)\n    return transcription[\"text\"].strip()\n</code></pre>"},{"location":"stt_utils/","title":"Speech to text utils","text":""},{"location":"stt_utils/#stt.utils","title":"stt.utils","text":"<p>Functions:</p> Name Description <code>make_stream</code> <code>record_interruption_parallel</code> <code>record_interruption</code> <code>record_user</code> <code>record_user_stream</code> <p>Attributes:</p> Name Type Description <code>CHUNK</code> <code>FORMAT</code> <code>CHANNELS</code> <code>RATE</code>"},{"location":"stt_utils/#stt.utils.CHUNK","title":"CHUNK  <code>module-attribute</code>","text":"<pre><code>CHUNK = int(1024 * 2)\n</code></pre>"},{"location":"stt_utils/#stt.utils.FORMAT","title":"FORMAT  <code>module-attribute</code>","text":"<pre><code>FORMAT = paInt16\n</code></pre>"},{"location":"stt_utils/#stt.utils.CHANNELS","title":"CHANNELS  <code>module-attribute</code>","text":"<pre><code>CHANNELS = 1\n</code></pre>"},{"location":"stt_utils/#stt.utils.RATE","title":"RATE  <code>module-attribute</code>","text":"<pre><code>RATE = 16000\n</code></pre>"},{"location":"stt_utils/#stt.utils.make_stream","title":"make_stream","text":"<pre><code>make_stream()\n</code></pre> Source code in <code>openvoicechat/stt/utils.py</code> <pre><code>def make_stream():\n    p = pyaudio.PyAudio()\n    return p.open(\n        format=FORMAT, channels=CHANNELS, rate=RATE, input=True, frames_per_buffer=CHUNK\n    )\n</code></pre>"},{"location":"stt_utils/#stt.utils.record_interruption_parallel","title":"record_interruption_parallel","text":"<pre><code>record_interruption_parallel(vad, listen_queue)\n</code></pre> Source code in <code>openvoicechat/stt/utils.py</code> <pre><code>def record_interruption_parallel(vad, listen_queue):\n    # listen for interruption untill the queue is not empty\n    frames = []\n    stream = make_stream()\n    while True:\n        a = listen_queue.get()\n        if a is None:\n            break\n        data = stream.read(CHUNK)\n        frames.append(data)\n        contains_speech = vad.contains_speech(frames[int(RATE / CHUNK) * -2 :])\n        if contains_speech:\n            stream.close()\n            frames = np.frombuffer(b\"\".join(frames), dtype=np.int16)\n            frames = frames / (1 &lt;&lt; 15)\n            return frames.astype(np.float32)\n    stream.close()\n    return None\n</code></pre>"},{"location":"stt_utils/#stt.utils.record_interruption","title":"record_interruption","text":"<pre><code>record_interruption(vad, record_seconds=100, streamer=None, logger=None)\n</code></pre> Source code in <code>openvoicechat/stt/utils.py</code> <pre><code>def record_interruption(vad, record_seconds=100, streamer=None, logger=None):\n    if logger:\n        logger.info(\n            \"recording for interruption\",\n            extra={\n                \"details\": \"record_interruption\",\n                \"further\": f\"{record_seconds} seconds\",\n            },\n        )\n    frames = []\n    if streamer is None:\n        stream = make_stream()\n        global CHUNK\n        global RATE\n    else:\n        stream = streamer.make_stream()\n        CHUNK = streamer.CHUNK\n        RATE = streamer.RATE\n\n    for _ in range(0, int(RATE / CHUNK * record_seconds)):\n        data = stream.read(CHUNK)\n        assert len(data) == CHUNK * 2, \"chunk size does not match 2 bytes per sample\"\n        frames.append(data)\n        contains_speech = vad.contains_speech(frames[int(RATE / CHUNK) * -2 :])\n        if contains_speech:\n            stream.close()\n            frames = np.frombuffer(b\"\".join(frames), dtype=np.int16)\n            frames = frames / (1 &lt;&lt; 15)\n            return frames.astype(np.float32)\n    stream.close()\n    return None\n</code></pre>"},{"location":"stt_utils/#stt.utils.record_user","title":"record_user","text":"<pre><code>record_user(silence_seconds, vad, streamer=None, started=False, logger=None)\n</code></pre> Source code in <code>openvoicechat/stt/utils.py</code> <pre><code>def record_user(silence_seconds, vad, streamer=None, started=False, logger=None):\n    frames = []\n\n    if streamer is None:\n        stream = make_stream()\n        global CHUNK\n        global RATE\n    else:\n        stream = streamer.make_stream()\n        CHUNK = streamer.CHUNK\n        RATE = streamer.RATE\n    one_second_iters = int(RATE / CHUNK)\n    if logger:\n        logger.info(\n            \"user recording started\",\n            extra={\"details\": \"record_user\", \"further\": f\"{silence_seconds} seconds\"},\n        )\n\n    while True:\n        data = stream.read(CHUNK)\n        assert len(data) == CHUNK * 2, \"chunk size does not match 2 bytes per sample\"\n        frames.append(data)\n        if len(frames) &lt; one_second_iters * silence_seconds:\n            continue\n        contains_speech = vad.contains_speech(\n            frames[int(-one_second_iters * silence_seconds) :]\n        )\n        if not started and contains_speech:\n            started = True\n            if logger:\n                logger.info(\n                    \"speech detected\",\n                    extra={\"details\": \"record_user\", \"further\": \"\"},\n                )\n        if started and contains_speech is False:\n            break\n    stream.close()\n    if logger:\n        logger.info(\n            \"user recording ended\",\n            extra={\"details\": \"record_user\", \"further\": \"\"},\n        )\n\n    # creating a np array from buffer\n    frames = np.frombuffer(b\"\".join(frames), dtype=np.int16)\n\n    # normalization see https://discuss.pytorch.org/t/torchaudio-load-normalization-question/71470\n    frames = frames / (1 &lt;&lt; 15)\n\n    return frames.astype(np.float32)\n</code></pre>"},{"location":"stt_utils/#stt.utils.record_user_stream","title":"record_user_stream","text":"<pre><code>record_user_stream(silence_seconds, vad, audio_queue, streamer=None)\n</code></pre> Source code in <code>openvoicechat/stt/utils.py</code> <pre><code>def record_user_stream(silence_seconds, vad, audio_queue, streamer=None):\n    frames = []\n\n    started = False\n    if streamer is None:\n        stream = make_stream()\n        global CHUNK\n        global RATE\n    else:\n        stream = streamer.make_stream()\n        CHUNK = streamer.CHUNK\n        RATE = streamer.RATE\n\n    one_second_iters = int(RATE / CHUNK)\n    print(\"* recording\")\n\n    while True:\n        data = stream.read(CHUNK)\n        assert len(data) == CHUNK * 2, \"chunk size does not match 2 bytes per sample\"\n        frames.append(data)\n        audio_queue.put(data)\n        contains_speech = vad.contains_speech(\n            frames[int(-one_second_iters * silence_seconds) :]\n        )\n        if not started and contains_speech:\n            started = True\n            print(\"*listening to speech*\")\n        if started and contains_speech is False:\n            break\n    audio_queue.put(None)\n    stream.close()\n    print(\"* done recording\")\n</code></pre>"},{"location":"stt_utils/#stt.vad.VoiceActivityDetection","title":"stt.vad.VoiceActivityDetection","text":"<pre><code>VoiceActivityDetection(sampling_rate=16000)\n</code></pre> <p>Methods:</p> Name Description <code>contains_speech</code> <p>Attributes:</p> Name Type Description <code>sampling_rate</code> Source code in <code>openvoicechat/stt/vad.py</code> <pre><code>def __init__(self, sampling_rate=16000):\n    self.model, utils = torch.hub.load(\n        repo_or_dir=\"snakers4/silero-vad\", model=\"silero_vad\", force_reload=False\n    )\n\n    (\n        self.get_speech_timestamps,\n        self.save_audio,\n        self.read_audio,\n        self.VADIterator,\n        self.collect_chunks,\n    ) = utils\n\n    self.sampling_rate = sampling_rate\n</code></pre>"},{"location":"stt_utils/#stt.vad.VoiceActivityDetection.sampling_rate","title":"sampling_rate  <code>instance-attribute</code>","text":"<pre><code>sampling_rate = sampling_rate\n</code></pre>"},{"location":"stt_utils/#stt.vad.VoiceActivityDetection.contains_speech","title":"contains_speech","text":"<pre><code>contains_speech(audio)\n</code></pre> Source code in <code>openvoicechat/stt/vad.py</code> <pre><code>def contains_speech(self, audio):\n    frames = np.frombuffer(b\"\".join(audio), dtype=np.int16)\n\n    # normalization see https://discuss.pytorch.org/t/torchaudio-load-normalization-question/71470\n    frames = frames / (1 &lt;&lt; 15)\n\n    audio = torch.tensor(frames.astype(np.float32))\n    speech_timestamps = self.get_speech_timestamps(\n        audio, self.model, sampling_rate=self.sampling_rate\n    )\n    return len(speech_timestamps) &gt; 0\n</code></pre>"},{"location":"tts_classes/","title":"Text to speech classes","text":""},{"location":"tts_classes/#tts.tts_xtts.Mouth_xtts","title":"tts.tts_xtts.Mouth_xtts","text":"<pre><code>Mouth_xtts(model_id='tts_models/en/jenny/jenny', device='cpu', player=sd, speaker=None, wait=True, logger=None)\n</code></pre> <p>               Bases: <code>BaseMouth</code></p> <p>Methods:</p> Name Description <code>run_tts</code> <p>Attributes:</p> Name Type Description <code>model</code> <code>device</code> <code>speaker</code> Source code in <code>openvoicechat/tts/tts_xtts.py</code> <pre><code>def __init__(\n    self,\n    model_id=\"tts_models/en/jenny/jenny\",\n    device=\"cpu\",\n    player=sd,\n    speaker=None,\n    wait=True,\n    logger=None,\n):\n    from TTS.api import TTS\n\n    self.model = TTS(model_id)\n    self.device = device\n    self.model.to(device)\n    self.speaker = speaker\n    super().__init__(\n        sample_rate=self.model.synthesizer.output_sample_rate,\n        player=player,\n        wait=wait,\n        logger=logger,\n    )\n</code></pre>"},{"location":"tts_classes/#tts.tts_xtts.Mouth_xtts.model","title":"model  <code>instance-attribute</code>","text":"<pre><code>model = TTS(model_id)\n</code></pre>"},{"location":"tts_classes/#tts.tts_xtts.Mouth_xtts.device","title":"device  <code>instance-attribute</code>","text":"<pre><code>device = device\n</code></pre>"},{"location":"tts_classes/#tts.tts_xtts.Mouth_xtts.speaker","title":"speaker  <code>instance-attribute</code>","text":"<pre><code>speaker = speaker\n</code></pre>"},{"location":"tts_classes/#tts.tts_xtts.Mouth_xtts.run_tts","title":"run_tts","text":"<pre><code>run_tts(text)\n</code></pre> Source code in <code>openvoicechat/tts/tts_xtts.py</code> <pre><code>def run_tts(self, text):\n    output = self.model.tts(\n        text=text,\n        split_sentences=False,\n        speaker=self.speaker,\n        language=\"en\" if self.model.is_multi_lingual else None,\n    )\n    return np.array(output)\n</code></pre>"},{"location":"tts_classes/#tts.tts_elevenlabs.Mouth_elevenlabs","title":"tts.tts_elevenlabs.Mouth_elevenlabs","text":"<pre><code>Mouth_elevenlabs(model_id='eleven_turbo_v2', voice_id='IKne3meq5aSn9XLyUdCD', api_key='', player=sd, wait=True, logger=None)\n</code></pre> <p>               Bases: <code>BaseMouth</code></p> <p>Methods:</p> Name Description <code>run_tts</code> <p>Attributes:</p> Name Type Description <code>model_id</code> <code>voice_id</code> <code>api_key</code> Source code in <code>openvoicechat/tts/tts_elevenlabs.py</code> <pre><code>def __init__(\n    self,\n    model_id=\"eleven_turbo_v2\",\n    voice_id=\"IKne3meq5aSn9XLyUdCD\",\n    api_key=\"\",\n    player=sd,\n    wait=True,\n    logger=None,\n):\n    self.model_id = model_id\n    self.voice_id = voice_id\n    if api_key == \"\":\n        load_dotenv()\n        api_key = os.getenv(\"ELEVENLABS_API_KEY\")\n    self.api_key = api_key\n    super().__init__(sample_rate=44100, player=player, wait=wait, logger=logger)\n</code></pre>"},{"location":"tts_classes/#tts.tts_elevenlabs.Mouth_elevenlabs.model_id","title":"model_id  <code>instance-attribute</code>","text":"<pre><code>model_id = model_id\n</code></pre>"},{"location":"tts_classes/#tts.tts_elevenlabs.Mouth_elevenlabs.voice_id","title":"voice_id  <code>instance-attribute</code>","text":"<pre><code>voice_id = voice_id\n</code></pre>"},{"location":"tts_classes/#tts.tts_elevenlabs.Mouth_elevenlabs.api_key","title":"api_key  <code>instance-attribute</code>","text":"<pre><code>api_key = api_key\n</code></pre>"},{"location":"tts_classes/#tts.tts_elevenlabs.Mouth_elevenlabs.run_tts","title":"run_tts","text":"<pre><code>run_tts(text)\n</code></pre> Source code in <code>openvoicechat/tts/tts_elevenlabs.py</code> <pre><code>def run_tts(self, text):\n    url = f\"https://api.elevenlabs.io/v1/text-to-speech/{self.voice_id}?optimize_streaming_latency=4\"\n    headers = {\n        \"Accept\": \"audio/mpeg\",\n        \"Content-Type\": \"application/json\",\n        \"xi-api-key\": f\"{self.api_key}\",\n    }\n\n    data = {\n        \"text\": text,\n        \"model_id\": self.model_id,\n        \"voice_settings\": {\"stability\": 0.5, \"similarity_boost\": 0.5},\n    }\n\n    response = requests.post(url, json=data, headers=headers)\n    audio_segment = AudioSegment.from_file(\n        io.BytesIO(response.content), format=\"mp3\"\n    )\n\n    samples = np.array(audio_segment.get_array_of_samples())\n\n    return samples\n</code></pre>"},{"location":"tts_classes/#tts.tts_hf.Mouth_hf","title":"tts.tts_hf.Mouth_hf","text":"<pre><code>Mouth_hf(model_id='kakao-enterprise/vits-vctk', device='cpu', forward_params={'speaker_id': 10}, player=sd, wait=True, logger=None)\n</code></pre> <p>               Bases: <code>BaseMouth</code></p> <p>Methods:</p> Name Description <code>run_tts</code> <p>Attributes:</p> Name Type Description <code>pipe</code> <code>device</code> <code>forward_params</code> Source code in <code>openvoicechat/tts/tts_hf.py</code> <pre><code>def __init__(\n    self,\n    model_id=\"kakao-enterprise/vits-vctk\",\n    device=\"cpu\",\n    forward_params={\"speaker_id\": 10},\n    player=sd,\n    wait=True,\n    logger=None,\n):\n    from transformers import pipeline\n\n    self.pipe = pipeline(\"text-to-speech\", model=model_id, device=device)\n    self.device = device\n    self.forward_params = forward_params\n    super().__init__(\n        sample_rate=self.pipe.sampling_rate, player=player, wait=wait, logger=logger\n    )\n</code></pre>"},{"location":"tts_classes/#tts.tts_hf.Mouth_hf.pipe","title":"pipe  <code>instance-attribute</code>","text":"<pre><code>pipe = pipeline('text-to-speech', model=model_id, device=device)\n</code></pre>"},{"location":"tts_classes/#tts.tts_hf.Mouth_hf.device","title":"device  <code>instance-attribute</code>","text":"<pre><code>device = device\n</code></pre>"},{"location":"tts_classes/#tts.tts_hf.Mouth_hf.forward_params","title":"forward_params  <code>instance-attribute</code>","text":"<pre><code>forward_params = forward_params\n</code></pre>"},{"location":"tts_classes/#tts.tts_hf.Mouth_hf.run_tts","title":"run_tts","text":"<pre><code>run_tts(text)\n</code></pre> Source code in <code>openvoicechat/tts/tts_hf.py</code> <pre><code>def run_tts(self, text):\n    with torch.no_grad():\n        # inputs = self.tokenizer(text, return_tensors=\"pt\")\n        # inputs = inputs.to(self.device)\n        # output = self.model(**inputs, speaker_id=self.speaker_id).waveform[0].to('cpu')\n        output = self.pipe(text, forward_params=self.forward_params)\n        self.sample_rate = output[\"sampling_rate\"]\n        return output[\"audio\"][0]\n</code></pre>"},{"location":"tts_classes/#tts.tts_piper.Mouth_piper","title":"tts.tts_piper.Mouth_piper","text":"<pre><code>Mouth_piper(device='cpu', model_path='models/en_US-ryan-high.onnx', config_path='models/en_en_US_ryan_high_en_US-ryan-high.onnx.json', player=sd, wait=True, logger=None)\n</code></pre> <p>               Bases: <code>BaseMouth</code></p> <p>Methods:</p> Name Description <code>run_tts</code> <p>Attributes:</p> Name Type Description <code>model</code> Source code in <code>openvoicechat/tts/tts_piper.py</code> <pre><code>def __init__(\n    self,\n    device=\"cpu\",\n    model_path=\"models/en_US-ryan-high.onnx\",\n    config_path=\"models/en_en_US_ryan_high_en_US-ryan-high.onnx.json\",\n    player=sd,\n    wait=True,\n    logger=None,\n):\n    import piper\n\n    self.model = piper.PiperVoice.load(\n        model_path=model_path,\n        config_path=config_path,\n        use_cuda=True if device == \"cuda\" else False,\n    )\n    super().__init__(\n        sample_rate=self.model.config.sample_rate,\n        player=player,\n        wait=wait,\n        logger=logger,\n    )\n</code></pre>"},{"location":"tts_classes/#tts.tts_piper.Mouth_piper.model","title":"model  <code>instance-attribute</code>","text":"<pre><code>model = load(model_path=model_path, config_path=config_path, use_cuda=True if device == 'cuda' else False)\n</code></pre>"},{"location":"tts_classes/#tts.tts_piper.Mouth_piper.run_tts","title":"run_tts","text":"<pre><code>run_tts(text)\n</code></pre> Source code in <code>openvoicechat/tts/tts_piper.py</code> <pre><code>def run_tts(self, text):\n    audio = b\"\"\n    for i in self.model.synthesize_stream_raw(text):\n        audio += i\n    return np.frombuffer(audio, dtype=np.int16)\n</code></pre>"},{"location":"utils/","title":"Utils","text":""},{"location":"utils/#utils","title":"utils","text":"<p>Functions:</p> Name Description <code>run_chat</code> <p>Runs a chat session between a user and a bot.</p>"},{"location":"utils/#utils.run_chat","title":"run_chat","text":"<pre><code>run_chat(mouth, ear, chatbot, verbose=True, stopping_criteria=lambda x: False, starting_message='')\n</code></pre> <p>Runs a chat session between a user and a bot. The function works by continuously listening to the user's input and generating the bot's responses in separate threads. If the user interrupts the bot's speech, the remaining part of the bot's response is saved and prepended to the user's next input. The chat stops when the stopping_criteria function returns True for a bot's response.</p> <p>Parameters:</p> Name Type Description Default <code>mouth</code> <p>A mouth object.</p> required <code>ear</code> <p>An ear object.</p> required <code>chatbot</code> <p>A chatbot object.</p> required <code>stopping_criteria</code> <p>A function that determines when the chat should stop. It takes the bot's response as input and returns a boolean. Defaults to a function that always returns False.</p> <code>lambda x: False</code> Source code in <code>openvoicechat/utils.py</code> <pre><code>def run_chat(\n    mouth,\n    ear,\n    chatbot,\n    verbose=True,\n    stopping_criteria=lambda x: False,\n    starting_message=\"\",\n):\n    \"\"\"\n    Runs a chat session between a user and a bot.\n    The function works by continuously listening to the user's input and generating the bot's responses in separate\n    threads. If the user interrupts the bot's speech, the remaining part of the bot's response is saved and prepended\n    to the user's next input. The chat stops when the stopping_criteria function returns True for a bot's response.\n\n    :param mouth: A mouth object.\n    :param ear: An ear object.\n    :param chatbot: A chatbot object.\n    :param stopping_criteria: A function that determines when the chat should stop. It takes the bot's response as input and returns a boolean. Defaults to a function that always returns False.\n    \"\"\"\n\n    if starting_message:\n        mouth.say_text(starting_message)\n\n    pre_interruption_text = \"\"\n    while True:\n        user_input = pre_interruption_text + \" \" + ear.listen()\n\n        if verbose:\n            print(\"USER: \", user_input)\n\n        llm_output_queue = queue.Queue()\n        interrupt_queue = queue.Queue()\n        llm_thread = threading.Thread(\n            target=chatbot.generate_response_stream,\n            args=(user_input, llm_output_queue, interrupt_queue),\n        )\n        tts_thread = threading.Thread(\n            target=mouth.say_multiple_stream,\n            args=(llm_output_queue, ear.interrupt_listen, interrupt_queue),\n        )\n\n        llm_thread.start()\n        tts_thread.start()\n\n        llm_thread.join()\n        tts_thread.join()\n        if not interrupt_queue.empty():\n            pre_interruption_text = interrupt_queue.get()\n        else:\n            pre_interruption_text = \"\"\n\n        res = llm_output_queue.get()\n        if stopping_criteria(res):\n            break\n        if verbose:\n            print(\"BOT: \", res)\n</code></pre>"},{"location":"getting-started/installation/","title":"Installation","text":""},{"location":"getting-started/installation/#pip-installation","title":"pip installation","text":"<pre><code>pip install openvoicechat\n</code></pre>"},{"location":"getting-started/installation/#other-requirements","title":"Other Requirements","text":"<ul> <li>portaudio</li> <li>torch</li> <li>torchaudio</li> </ul>"},{"location":"getting-started/installation/#install-model-specific-packages","title":"Install model specific packages","text":"Category Model Name Required Packages TTS Piper <code>pip install piper-tts piper-phonemize</code> TTS xtts - Coqui <code>pip install TTS phonemizer</code> ALL transformers - HuggingFace <code>pip install transformers</code> LLM Ollama <code>pip install ollama</code> LLM OpenAI <code>pip install openai</code> <p>Below you can select the required packages, and the <code>pip install</code> command will be generated automatically:</p> Select Required Packages HuggingFace - transformers Ollama OpenAI Piper-tts xtts <pre><code>pip install"},{"location":"getting-started/quickstart/","title":"Quickstart","text":"<p>Talk to an apple sales agent.</p> <pre><code>from openvoicechat.tts.tts_xtts import Mouth_xtts\nfrom openvoicechat.llm.llm_ollama import Chatbot_ollama\nfrom openvoicechat.stt.stt_hf import Ear_hf\nfrom openvoicechat.utils import run_chat\nfrom openvoicechat.llm.prompts import llama_sales\nimport torch\nfrom dotenv import load_dotenv\nimport os\n\n\nif __name__ == \"__main__\":\n    if torch.backends.mps.is_available():\n        device = \"mps\"\n    elif torch.cuda.is_available():\n        device = \"cuda\"\n    else:\n        device = \"cpu\"\n\n    print(\"loading models... \", device)\n    load_dotenv()\n    ear = Ear_hf(\n        model_id=\"openai/whisper-tiny.en\",\n        silence_seconds=1.5,\n        device=device,\n        listen_interruptions=False,\n    )\n\n    chatbot = Chatbot_ollama(sys_prompt=llama_sales, model=\"qwen2:0.5b\")\n\n    mouth = Mouth_xtts(device=device)\n\n    run_chat(\n        mouth, ear, chatbot, verbose=True, stopping_criteria=lambda x: \"[END]\" in x\n    )\n</code></pre>"}]}